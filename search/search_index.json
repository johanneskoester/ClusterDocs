{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IKIM Scientific Computing","text":"<p>This website contains documentation for the scientific computing infrastructure at the Institute for AI in Medicine (IKIM) in Essen. The documentation is geared towards researchers and students that aim to run scientific experiments on the cluster. See Getting Started for general instructions.</p>"},{"location":"#about","title":"About","text":"<p>The sources of this documentation can be found on GitHub and we encourage contribution.</p>"},{"location":"apptainer/","title":"Creating containers with Apptainer","text":"<p>Apptainer is the container system installed on the Slurm cluster. In contrast to Docker, it does not grant the user elevated privileges and is well-suited to multi-user environments. Nonetheless, it can run containers from Docker images as long as they do not depend on intrinsic root privileges such as binding to host ports below 1024.</p>"},{"location":"apptainer/#basic-commands","title":"Basic commands","text":"<p>Apptainer provides the following commands for launching containers:</p> <ul> <li><code>apptainer run</code>: executes the default startup command.</li> <li><code>apptainer exec</code>: executes a custom command.</li> <li><code>apptainer shell</code>: executes an interactive shell.</li> <li><code>apptainer instance start</code>: executes a background service.</li> </ul> <p>The above commands support mostly the same set of options.</p>"},{"location":"apptainer/#executing-a-docker-image","title":"Executing a Docker image","text":"<p>When using the prefix <code>docker://</code>, Apptainer pulls the image from Docker Hub.</p> <pre><code>apptainer run docker://alpine\n</code></pre> <p>The default execution model of Apptainer is different from Docker's. The container filesystem in Apptainer is read-only, although a number of paths such as <code>/tmp</code>, the user's home and the current directory are mounted read-write from the host into the container. Additionally, the default container user is the host user rather than root. The following examples demonstrate the effects of this behavior.</p> <pre><code># When creating a file in the container, it is owned by the current user both in the container and on the host.\nalice@c1:~$ apptainer exec \\\ndocker://alpine \\\nsh -c 'touch hello &amp;&amp; ls -l hello'\n-rw-rw-r--    1 alice   alice           0 Apr 13 10:07 hello\nalice@c1:~$ ls -l hello\n-rw-rw-r-- 1 alice alice 0 Apr 13 10:07 hello\n</code></pre> <pre><code># Non-mounted paths are read-only.\nalice@c1:~$ apptainer exec \\\ndocker://alpine \\\napk add busybox-extras\nERROR: Unable to lock database: Read-only file system\nERROR: Failed to open apk database: Read-only file system\n</code></pre> <p>The immutability of non-mounted paths, in combination with executing as the host user, makes it convenient to run multiple containers in parallel on the cluster with easy access to NFS storage. If this is not desired, other modes of operation are listed below.</p>"},{"location":"apptainer/#bind-mounting","title":"Bind-mounting","text":"<p>If a Docker image expects to be able to write in specific locations, they can simply be mounted in the container while preserving the read-only mode for the rest of the filesystem. For example:</p> <pre><code>mkdir ~/pgrun ~/pgdata\napptainer run \\\n--bind ~/pgdata:/var/lib/postgresql/data \\\n--bind ~/pgrun:/var/run/postgresql \\\n--env POSTGRES_PASSWORD=secret \\\ndocker://postgres\n</code></pre>"},{"location":"apptainer/#writable-tmpfs","title":"Writable tmpfs","text":"<p>The option <code>--writable-tmpfs</code> creates a writable area that allows making changes to the container filesystem. The default size is 16 MiB, therefore it is only suitable for small writes such as PID or state-tracking files. Any changes are lost when the container exits.</p> <pre><code>mkdir ~/pgdata\napptainer run \\\n--bind ~/pgdata:/var/lib/postgresql/data \\\n--writable-tmpfs \\\n--env POSTGRES_PASSWORD=secret \\\ndocker://postgres\n</code></pre>"},{"location":"apptainer/#sandbox","title":"Sandbox","text":"<p>Apptainer can switch to a full read-write model by combining sandbox directories with fakeroot mode. A sandbox is a filesystem tree in a directory on the host. When executing a container from a sandbox, the filesystem can be made writable. Fakeroot mode makes the user appear as root in the container, thereby allowing complete access to the container filesystem.</p> <pre><code># Create a sandbox on local storage.\nalice@c1:~$ apptainer build --sandbox /local/work/mysandbox docker://alpine\nalice@c1:~$ ls /local/work/mysandbox\nbin  dev  environment  etc  home  lib  media  mnt  opt  proc  root  run  sbin  singularity  srv  sys  tmp  usr  var\n\n# Install a package in the sandbox.\nalice@c1:~$ apptainer exec \\\n--writable \\\n--fakeroot \\\n/local/work/mysandbox \\\napk add busybox-extras\n</code></pre> <p>Fakeroot mode cannot work properly if the sandbox directory is on NFS. For best results, sandboxes should be placed on local storage as shown in the example above.</p> <p>Depending on the changes made to a sandbox, it might not be possible to delete it from the host as a regular user, although it can always be deleted from a fakeroot container:</p> <pre><code># Remove the contents of the sandbox.\napptainer exec --fakeroot --bind /local/work/mysandbox docker://alpine rm -R /local/work/mysandbox\n\n# Remove the leftover empty directory.\nrmdir /local/work/mysandbox\n</code></pre>"},{"location":"apptainer/#using-gpus","title":"Using GPUs","text":"<p>The <code>--nv</code> option instructs Apptainer to add GPU support to the container.</p> <pre><code>apptainer exec --nv docker://nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 /usr/bin/nvidia-smi\n</code></pre> <p>Apptainer doesn't have an equivalent of the <code>--gpus</code> option from the NVIDIA Docker runtime. The environment variable <code>CUDA_VISIBLE_DEVICES</code> should be used to control GPU visibility inside the container. See Targeting GPU nodes with Slurm.</p>"},{"location":"conda/","title":"Mamba/Conda","text":"<p>Mambaforge provides <code>mamba</code> and <code>conda</code> on the cluster. Users who prefer to manage their own installation can install a Conda distribution in their home directory.</p> <p>See Getting started with conda for an introduction.</p>"},{"location":"conda/#using-conda-with-slurm","title":"Using Conda with Slurm","text":"<p>Submitting a slurm job which includes <code>conda activate</code> will result in the following error:</p> <pre><code>$ srun -N1 conda activate myenv\n\nCommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n</code></pre> <p>This happens because <code>conda activate</code> is made available as a shell function which is not preserved when slurm executes a job.</p> <p>The suggestion of executing <code>conda init</code> does not apply to Slurm. Instead, an environment can be activated with one of the following methods:</p> <ul> <li>Execute <code>conda activate</code> while logged into the submission node, then submit the job:</li> </ul> <pre><code>$ conda activate myenv\n(myenv) $ srun -N1 env | grep CONDA_PREFIX\nCONDA_PREFIX=/homes/user/.conda/envs/myenv\n</code></pre> <ul> <li>Execute <code>eval \"$(conda shell.bash hook)\"</code> as part of the slurm job in order to make <code>conda activate</code> available:</li> </ul> <pre><code>$ cat run.sh\n#!/usr/bin/env bash\n#SBATCH -N1\neval \"$(conda shell.bash hook)\"\nconda activate myenv\nsrun -N1 env | grep CONDA_PREFIX\n\n$ sbatch run.sh\n\n$ cat slurm-120651.out\nCONDA_PREFIX=/homes/user/.conda/envs/myenv\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome the IKIM cluster documentation. The goal of this document is to give you enough background to work on the IKIM cluster. It is not meant as a general introduction to remote computing services. We will refer to external sources where necessary.</p> <p>If you have any questions, please reach out to your project coordinator for help.</p>"},{"location":"getting-started/#getting-cluster-access","title":"Getting cluster access","text":"<p>Note: we assume that you are using a Linux or MacOS operating system. If you are using Windows, please see below for recommendations.</p> <p>To get access to the IKIM computing infrastructure you need an SSH key. Use the command below to create your SSH key. When prompted, make sure to choose a strong passphrase and save the passphrase in your password manager.</p> <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/id_ikim\n</code></pre> <p>Please send the public key along with following contact details to your project coordinator:</p> <ul> <li>First name</li> <li>Last name</li> <li>Email address (domain uk-essen.de or uni-due.de if available)</li> <li>Public SSH key (<code>~/.ssh/id_ikim.pub</code>)</li> </ul> <p>Afterwards, an account will be created for you in the central user management. When this is done, you should be able to SSH into the cluster.</p> Example: output of SSH-keypair generation.  When executing the command above, you should should see output similar to this:  <pre><code>Generating public/private ed25519 key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /Users/&lt;user&gt;/.ssh/id_ikim\nYour public key has been saved in /Users/&lt;user&gt;/.ssh/id_ikim.pub\nThe key fingerprint is:\nSHA256:PQyNrogYs001Y0IlsG75teDBFVlDmd7xSJPNI1lrQr4 user@&lt;host&gt;\nThe key's randomart image is:\n+--[ED25519 256]--+\n|..o...++o.*.     |\n| o . ..o+X +.    |\n|. . =.. =oBo.    |\n|. o+.o o *+.     |\n|o+.+ .  SE+      |\n|.Bo.+...   .     |\n|o oo...          |\n|                 |\n|                 |\n+----[SHA256]-----+\n</code></pre>  Note that two files were created in your home directory in the `.ssh` subdirectory:  <pre><code>$ ls ~/.ssh\nconfig  id_ikim  id_ikim.pub  known_hosts\n</code></pre>  - `~/.ssh/id_ikim` - This is your private SSH key. Treat this file like a password. Do not share it with anyone. - `~/.ssh/id_ikim.pub` - This is your public SSH key. This should be shared with your project coordinator. You can open it with any text editor.  The contents of `~/.ssh/id_ikim.pub` look similar to this:  <pre><code>$ cat ~/.ssh/id_ikim.pub\nssh-ed25519 [long random string] &lt;user&gt;@&lt;host&gt;\n</code></pre>"},{"location":"getting-started/#setting-up-your-ssh-configuration","title":"Setting up your SSH configuration","text":"<p>To provide the appropriate parameters for the connection, create a file at <code>~/.ssh/config</code> and copy the snippet below, replacing <code>$USERNAME</code> appropriately.</p> <pre><code>Host *\n  AddKeysToAgent yes\n  CanonicalizeHostname yes\n\nHost ikim\n  HostName login.ikim.uk-essen.de\n  User $USERNAME\n  IdentityFile ~/.ssh/id_ikim\n  ForwardAgent yes\n\nHost g1-? c? c?? slurmq\n  Hostname %h.ikim.uk-essen.de\n  User $USERNAME\n  IdentityFile ~/.ssh/id_ikim\n  ProxyJump ikim\n  ForwardAgent yes\n</code></pre> <p>With <code>~/.ssh/config</code> in place, you can use the following command to log into any host, replacing <code>$HOSTNAME</code> appropriately.</p> <pre><code>ssh $HOSTNAME\n</code></pre> <p>The connection will transparently jump through the host labeled <code>ikim</code> and proceed to the target <code>$HOSTNAME</code>. The <code>ikim</code> host itself must not be used for computational work.</p>"},{"location":"getting-started/#test-your-ssh-login","title":"Test your SSH login","text":"<p>Try the example below to test that your SSH client is properly configured:</p> <pre><code># Log into the slurm submission node\nssh slurmq\n</code></pre> <p>If the above is not working, please run the commands below and send the debug messages to your project coordinator for help.</p> <pre><code>ssh -v slurmq\n</code></pre> <pre><code>ssh -v ikim\n</code></pre>"},{"location":"getting-started/#ssh-clients-on-windows","title":"SSH clients on Windows","text":"<p>We recommend two options for installing and using an SSH client on Windows:</p> <ul> <li>Windows Subsystem for Linux (WSL2) provides Linux distributions running in a lightweight virtual machine on Windows. With WSL, the instructions above can be followed without changes and the default shell environment is identical to the one found on IKIM hosts.</li> <li>OpenSSH is the same software suite that comes preinstalled on other operating systems. To install it, go to the Apps &amp; Features settings page and select Optional Features, then add the OpenSSH Client feature. The instructions above should work simply by adapting paths to Windows-style. Older clients might produce an error message that starts with \"Bad stdio forwarding specification\", which can be fixed by replacing the <code>ProxyJump</code> directive with:</li> </ul> <pre><code>ProxyCommand ssh.exe -W %h:%p ikim\n</code></pre>"},{"location":"getting-started/#what-hardware-is-available-on-the-ikim-cluster","title":"What hardware is available on the IKIM cluster?","text":"<p>The cluster has two sets of servers: 120 nodes for CPU-bound tasks and 10 nodes for GPU-bound tasks. At this moment, not all of these nodes are available for general computation tasks. However, more will be added in future. The following hardware is installed in the servers:</p> <ul> <li>CPU nodes (<code>c1</code> - <code>c120</code>): Each with 192GB RAM, 2 CPU Intel, 1 SSD for system and 1 SSD for data (2TB).</li> <li>GPU nodes (<code>g1-1</code> - <code>g1-10</code>): Each with 6 NVIDIA RTX 6000 GPUs, 1024GB RAM, 2 CPU AMD, 1 SSD for system (1TB) and 2 NVMe for data (12TB configured as RAID-0).</li> <li>GPU node (<code>g2-1</code>): One node with 8 NVIDIA A100 80G GPUs, 2TB RAM, 2 AMD EPYC CPUs (256 logical processors), 1 SSD for system (1TB) and 2 NVMe for data (30TB configured as RAID-0).</li> </ul> <p>A subset of these nodes are deployed as a Slurm cluster. Unless instructed otherwise, you should interact with worker nodes using Slurm.</p>"},{"location":"getting-started/#what-software-is-available-on-the-ikim-cluster","title":"What software is available on the IKIM cluster?","text":"<p>We aim to keep the computing environments on the cluster as clean as possible. Therefore, only commonly used software packages are pre-installed and configured. At this moment this includes:</p> <ul> <li>Slurm</li> <li>Python 3</li> <li>Mamba/Conda</li> <li>Apptainer</li> </ul> <p>Comprehensive guides on these tools are outside the scope of this document. To learn more, see the following resources:</p> <ul> <li>Slurm</li> <li>Conda Getting Started</li> <li>Apptainer User Guide</li> </ul>"},{"location":"getting-started/#where-to-store-your-data","title":"Where to store your data?","text":"<p>There are several locations where you can store data on the cluster:</p> <ul> <li>Your home directory (<code>/homes/&lt;username&gt;/</code>): This directory is only for personal data such as configuration files. Anything related to work or that should be visible to other people should not reside here.</li> <li>Project directory (<code>/projects/&lt;project_name&gt;/</code>): This location should be used for data related to your project. If you are starting a project, ask your project coordinator to create a directory and provide a list of participating users. Note that you cannot simply list all project directories via <code>ls /projects</code>; instead, you need to specify the full path, such as: <code>ls /projects/dso_mp_ws2021/</code></li> <li>Public dataset directory (<code>/projects/datashare</code>): A world-readable location for datasets for which no special access rights are required. To lower the risk of data loss, each user can write only in a subdirectory corresponding to their research group. For example, a user which belongs to group <code>tio</code> must add new datasets in <code>/projects/datashare/tio</code> but can browse and read throughout <code>/projects/datashare</code>.</li> <li>Group directory (<code>/groups/&lt;group_name&gt;</code>): This is the appropriate place for any data that should be shared within an IKIM research group. In student projects you will most likely not need group directories.</li> </ul> <p>All of the above directories (homes, projects, groups) are shared with other hosts on the cluster through the network file system (NFS). This is convenient: sharing data between hosts becomes effortless and your data is stored redundantly on the file server.</p> <p>Also see the storage for details and also info on performance.</p>"},{"location":"getting-started/#github-authentication-through-ssh","title":"GitHub Authentication through SSH","text":"<p>To clone GitHub repositories on the cluster over the <code>git+ssh</code> protocol, you need to (1) configure your local ssh client as per the GitHub documentation, and (2) enable agent forwarding (if you use the ssh config above, this should already be done). You can verify your setup with following command:</p> <pre><code>USER@g1-9:~$ ssh -T git@github.com\nHi USER! You've successfully authenticated, but GitHub does not provide shell access.\n</code></pre>"},{"location":"getting-started/#tips-on-working-with-remote-computing-services","title":"Tips on Working with remote computing services","text":"<ul> <li>Unix Crash Course</li> <li>Another Unix Course</li> <li>Tactical tmux: The 10 Most Important Commands</li> <li>How To Use Linux Screen</li> <li>Git Book</li> <li>Conda</li> </ul>"},{"location":"jupyter/","title":"Jupyter Notebook Workflow","text":"<p>The Jupyter Notebook is an open-source web application that allows creating and sharing documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Visit Jupyter.org for more information.</p>"},{"location":"jupyter/#starting-a-jupyter-server-instance","title":"Starting a Jupyter server instance","text":"<p>To launch a Jupyter instance on the Slurm cluster, first log into the Slurm submission node:</p> <pre><code>ssh slurmq\n</code></pre> <p>Next, either create a conda environment with the package <code>notebook</code> from the channel <code>conda-forge</code> or install the package in an existing environment.</p> <pre><code>conda create -n myproject -c conda-forge notebook\nconda activate myproject\n</code></pre> <p>The Jupyter startup command must be submitted to Slurm using <code>srun</code> or <code>sbatch</code>. Here is a minimal example which launches the server on a worker node with 32 allocated CPU cores and an 8-hour deadline:</p> <pre><code>srun --time 08:00:00 --cpus-per-task=32 jupyter notebook --ip 0.0.0.0 --no-browser\n</code></pre> <p>The hostname, port number and authorization token are displayed upon startup. For example:</p> <pre><code>[I 06:30:19.290 NotebookApp] Serving notebooks from local directory: /homes/jan\n[I 06:30:19.290 NotebookApp] Jupyter Notebook 6.4.5 is running at:\n[I 06:30:19.290 NotebookApp] http://g1-7.ikim.uk-essen.de:8888/       ?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n[I 06:30:19.290 NotebookApp]  or http://127.0.0.1:8888/       ?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n[I 06:30:19.290 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice    to     skip confirmation).\n[C 06:30:19.296 NotebookApp]\n\n   To access the notebook, open this file in a browser:\n       file:///homes/jan/.local/share/jupyter/runtime/nbserver-426528-open.html\n   Or copy and paste one of these URLs:\n       http://g1-7.ikim.uk-essen.de:8888/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n    or http://127.0.0.1:8888/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n</code></pre> <p>The output above contains the following information:</p> <ul> <li>The hostname of the worker node: <code>g1-7</code>.</li> <li>The port number: <code>8888</code>.</li> <li>The authorization token: <code>d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9</code>.</li> </ul>"},{"location":"jupyter/#connecting-to-the-remote-jupyter-server","title":"Connecting to the remote Jupyter server","text":""},{"location":"jupyter/#from-a-browser-on-the-local-machine","title":"From a browser on the local machine","text":"<p>Worker nodes in the cluster are not exposed to the internet. In order to reach a cluster node from a local browser, an SSH tunnel through the login node must be launched. Open a terminal and execute the following command, replacing the hostname and port number from the example with the actual values:</p> <pre><code>ssh -N -L 8888:g1-7:8888 ikim\n</code></pre> <p>This command creates an SSH tunnel such that all traffic to <code>localhost:8888</code> passes through the host <code>ikim</code> and reaches the destination <code>g1-7:8888</code>. The terminal will simply hang, indicating that the tunnel has been created.</p> <p>Open a browser and paste the URL provided by Jupyter. Since the traffic needs to flow through the tunnel, replace the target host with <code>localhost</code>:</p> <pre><code>http://localhost:8888/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n</code></pre> <p>The Jupyter landing page should appear:</p> <p></p> <p>To verify that the notebook is running on the remote host and within the conda environment:</p> <p></p>"},{"location":"jupyter/#from-visual-studio-code","title":"From Visual Studio Code","text":"<p>Connect to the Slurm submission node as described in Connect to a remote host. When prompted for the target host, type <code>slurmq</code>.</p> <p>From the submission node, connect to the Jupyter server as described in Connect to a remote Jupyter server. When prompted for the URL, simply copy and paste the URL displayed in the output from <code>jupyter notebook</code>. As opposed to the browser method, in this case the connection originates from inside the cluster and there's no need to set up port forwarding.</p>"},{"location":"slurm/","title":"Slurm","text":"<p>The entry point to the Slurm cluster is the job submission node:</p> <pre><code>ssh slurmq\n</code></pre> <p>From the submission node, jobs (inline commands or scripts) can be submitted to worker nodes.</p> <p>Worker nodes are divided in groups called partitions in Slurm terminology. The default partition is made up of general-purpose CPU nodes.</p> <p>See Slurm quickstart for an introduction.</p>"},{"location":"slurm/#client-tools","title":"Client tools","text":"<p>This is a brief overview of the main commands available on the submission node. They can also be invoked from worker nodes as part of a submitted job.</p>"},{"location":"slurm/#sinfo","title":"sinfo","text":"<p>Use <code>sinfo</code> to display a summary of the worker nodes.</p>"},{"location":"slurm/#squeue","title":"squeue","text":"<p>Use <code>squeue</code> to display a list of scheduled jobs. It's generally useful to execute</p> <pre><code>squeue -l\n</code></pre> <p>to display additional information such as the time limit.</p>"},{"location":"slurm/#srun","title":"srun","text":"<p>Use <code>srun</code> to submit a job interactively. The output is displayed on the terminal.</p>"},{"location":"slurm/#example-obtaining-a-shell-on-a-node","title":"Example: obtaining a shell on a node","text":"<p>The following example opens a shell on a worker node. The node is selected automatically by Slurm.</p> <pre><code># A deadline is specified to avoid leaving a hanging session if the user doesn't terminate it.\nsrun --time=01:00:00 --pty bash -i\n</code></pre>"},{"location":"slurm/#example-requesting-resources","title":"Example: requesting resources","text":"<p>A simple <code>srun</code> command is executed by default on a single worker node, using a single CPU core. Several options are available for configuring the allocated resources. In the following example, Slurm allocates 32 CPU cores by picking a worker node with enough free cores.</p> <pre><code>srun --cpus-per-task=32 python3 script.py\n</code></pre>"},{"location":"slurm/#example-environment-inheritance","title":"Example: environment inheritance","text":"<p>When Slurm executes a job, it inherits the caller's environment and current directory. Slurm also sets several environment variables such as <code>SLURM_JOB_ID</code> and <code>SLURM_JOB_NAME</code>. The same applies to the other submission commands <code>sbatch</code> and <code>salloc</code>.</p> <p>To illustrate, let's create a script called <code>job.sh</code> and execute it via <code>srun</code>.</p> <pre><code>$ pwd\n/homes/alice/workdir\n\n$ cat job.sh\n#!/usr/bin/env sh\necho \"my job ran on $(date)\"\necho \"working directory: $(pwd)\"\necho \"FOO=$FOO\"  # this variable is not defined in the script\necho \"job id: $SLURM_JOB_ID\"  # this variable is defined by Slurm\necho $(hostname)\n\n# Define an environment variable and execute the job.\n$ FOO=bar srun sh job.sh\nmy job ran on Thu May  4 15:25:10 UTC 2023\nworking directory: /homes/alice/workdir\nFOO=bar\njob id: 340273\nc5.ikim.uk-essen.de\n</code></pre>"},{"location":"slurm/#sbatch","title":"sbatch","text":"<p>Use <code>sbatch</code> to submit a script. The output is written to a file in the current directory. All <code>srun</code> options apply to <code>sbatch</code> as well and can be included in the script itself with the special comment syntax <code>#SBATCH</code>.</p> <p><code>sbatch</code> allocates the requested resources, then executes the script on the first of the allocated nodes. To use the allocated resources effectively, <code>srun</code> must be invoked within the script, otherwise only one node is used.</p>"},{"location":"slurm/#example-submitting-a-job-with-sbatch","title":"Example: submitting a job with sbatch","text":"<p>Let's create a script called <code>job.sh</code> with a couple of <code>srun</code> calls and execute it via <code>sbatch</code>. Slurm redirects standard output to the text file <code>slurm-[job_id].out</code>.</p> <pre><code># Create a script which allocates 3 nodes and submits two job steps using srun.\n$ cat job.sh\n#!/usr/bin/env sh\n#SBATCH --nodes 3\n#SBATCH --time 01:00:00\nsrun hostname\nsrun echo hello\n\n# Execute the script.\n$ sbatch ./job.sh\nSubmitted batch job 340264\n\n# Display the output. The jobs steps were executed simultaneously on all 3 nodes.\n$ cat slurm-340264.out\nc7.ikim.uk-essen.de\nc23.ikim.uk-essen.de\nc20.ikim.uk-essen.de\nhello\nhello\nhello\n</code></pre> <p>The following example shows the effect of an equivalent script without <code>srun</code>.</p> <pre><code># Create a script which allocates 3 nodes, then executes two commands without srun.\n$ cat job.sh\n#!/usr/bin/env sh\n#SBATCH --nodes 3\n#SBATCH --time 01:00:00\nhostname\necho hello\n\n# Execute the script.\n$ sbatch ./job.sh\nSubmitted batch job 340266\n\n# Display the output. The commands were simply executed on the first assigned node.\n$ cat slurm-340266.out\nc7.ikim.uk-essen.de\nhello\n</code></pre>"},{"location":"slurm/#example-monitor-a-job","title":"Example: monitor a job","text":"<p>It can be useful to request a shell on a specific node, for example to monitor the resource usage of a running job.</p> <pre><code># Step 1: discover your allocated node with squeue.\nsqueue -l\n\n# Step 2: start an interactive shell on that node.\n# In this example, the job runs on node c120.\nsrun --nodelist=c120 --time=01:00:00 --pty bash -i\n\n# Step 3: run your diagnosis tools (e.g, htop, nvidia-smi, etc.)\n</code></pre> <p>Since any job requires at least one CPU core, this method doesn't work if all CPU cores on the target node have already been requested.</p>"},{"location":"slurm/#salloc","title":"salloc","text":"<p><code>salloc</code> allocates resources that can be used by subsequent invocations of <code>srun</code> or <code>sbatch</code>. By default, after allocating the resources <code>salloc</code> opens a shell on the current node: any <code>srun</code> or <code>sbatch</code> commands issued in this shell will use the allocated resources. When the user terminates the shell, the allocation is relinquished.</p> <pre><code>$ salloc --nodes=3 --time=01:00:00\nsalloc: Granted job allocation 340227\n\n$ srun hostname\nc5.ikim.uk-essen.de\nc6.ikim.uk-essen.de\nc7.ikim.uk-essen.de\n\n$ exit\nsalloc: Relinquishing job allocation 340227\nsalloc: Job allocation 340227 has been revoked.\n</code></pre> <p>If a command is supplied to <code>salloc</code>, it is executed instead of opening a shell. The command runs on the current node, therefore it should include <code>srun</code> or <code>sbatch</code>.</p> <p>The assigned nodes are not allocated exclusively unless specified, for example by requesting all available CPU cores.</p>"},{"location":"slurm/#scontrol","title":"scontrol","text":"<p>Use <code>scontrol</code> to display detailed information about a job such as the allocated resources and the times of submission/start.</p> <pre><code>scontrol show jobid -dd [job_id]\n</code></pre>"},{"location":"slurm/#scancel","title":"scancel","text":"<p>Use <code>scancel [job_id]</code> to cancel or terminate a job. Use <code>squeue</code> to display job IDs.</p>"},{"location":"slurm/#targeting-gpu-nodes","title":"Targeting GPU nodes","text":"<p>GPU nodes are available in a non-default partition named after the corresponding NVIDIA GPU architecture. The <code>--partition</code> option must be specified to target them.</p> <p>When executing GPU workloads, the <code>--gpus N</code> option should always be specified to let slurm assign <code>N</code> GPUS automatically. More specifically, Slurm automatically selects a suitable worker node, picks the specified number of GPUs randomly among the unassigned ones and sets the environment variable <code>CUDA_VISIBLE_DEVICES</code> accordingly.</p> <pre><code># In this example, 2 GPUs from a node in the GPUampere partition are exposed via CUDA_VISIBLE_DEVICES.\n# A deadline of 1 day and 12 hours is specified to let other users know when the GPUs will be available again.\nsrun --partition GPUampere --gpus 2 --time=1-12 train.py\n</code></pre> <p>By specifying <code>--gpus 0</code> or omitting the option, slurm does not set the <code>CUDA_VISIBLE_DEVICES</code> variable, but it does not mean that GPUs are hidden. The job will have access to all GPUs on the node, even the ones assigned to other slurm jobs. This is useful when monitoring the usage of resources on specific nodes:</p> <pre><code>srun --partition GPUampere --nodelist=g2-1 nvidia-smi\n</code></pre>"},{"location":"slurm/#job-submission-etiquette","title":"Job submission etiquette","text":"<p>If a job is expected to run continuously for many hours, a deadline should be specified with the option <code>--time</code>, even if just an overestimation. This information is especially valuable when all worker nodes are occupied as it allows other users to predict when their job will be scheduled. Accepted time formats include <code>minutes</code>, <code>minutes:seconds</code>, <code>hours:minutes:seconds</code>, <code>days-hours</code>, <code>days-hours:minutes</code> and <code>days-hours:minutes:seconds</code>.</p> <p>It's good practice to always specify a deadline when opening a shell (<code>srun --pty bash</code>). This avoids the \"hanging session\" issue that occurs if the user forgets to log out or loses the connection abruptly.</p>"},{"location":"storage/","title":"Storage on the IKIM cluster","text":"<p>The cluster has a number of options for retrieving and storing data. They have vastly different performance characteristics and greatly influence the time required to complete your computational analyses.</p>"},{"location":"storage/#classes-of-storage","title":"Classes of storage","text":"<p>Not all storage locations are alike and it is worth your while to understand their specific properties.</p>"},{"location":"storage/#local-storage-on-the-system-partition","title":"Local storage on the system partition","text":"<p>The local storage on each node typically consists of a system partition and a data partition. The system partition is used for the operating system, the configuration, swap files, pre-installed software. Most directories on the node are read-only to users.</p> location purpose user read-write status comment /etc/ configuration read-only /var/ temporary files read-only /var/tmp user-generated temporary files read-write local disk /tmp/ user-generated temporary files, deleted on reboot read-write local disk"},{"location":"storage/#local-storage-on-the-data-partition","title":"Local storage on the data partition","text":"<p>For some operations the NFS comes with unnecessary overhead. Therefore, the path <code>/local/work</code> is available for creating files and directories that reside on the data partition of the current host. This location should only be used for quick testing, preliminary experimentation and intermediate output. As soon as you need your files saved, move them to <code>/projects</code> or <code>/groups</code>. Local-only files are not backed up and can be deleted without notice.</p> <p>Here are tips on writing programs, scripts, containers, etc. that make good use of network resources:</p> <ul> <li>Read inputs from and write the final results to <code>/projects</code> or <code>/groups</code>.</li> <li>Write intermediate output to <code>/local/work</code>.</li> </ul>"},{"location":"storage/#nfs-storage","title":"NFS storage","text":"<p>Read operations on network storage (<code>/projects</code>, <code>/groups</code>) are cached transparently on local storage in the data partition. Generally speaking, your first access to a dataset will be slightly slower than usual due, but any subsequent access will be made from local storage.</p> <p>The file server has a 10Gib (10Gbs, 10 gigabit per second connection to the entire cluster. As a consequence each node can access a fraction of 10Gib, in the worst case a tiny fraction. However we note that a 250MB (megabyte) file will need a fraction of a second to transfer from the server to the client. This rather brilliant performance stats drastically change if and when random IO (as in not streaming large files, write-locking files, etc.) enter the equation. Those complex operations are best left to local disk.</p> <p>As a consequence, using local files or cached files is a good idea to ensure good runtime performance.</p> <p>Three different storage locations exist on the file server:</p> location purpose user read-write status comment /projects/ project data read-write not listable /groups/ group files read-write not listable /homes user home directory read-write not cached <p>Each user has a private home-directory. The contents of which are private to the userm typically no data relevant to any other user, project or your PI should be stored here.</p> <p>The projects directory provides a means to generate project specific storage, typically associated with a linux group shared by all members of the project. Thus <code>/projects/abc</code> is shared only by members of the project <code>abc</code>. We note that by using the <code>id</code> command users can identify all the groups they belong to. The contents of /projects are cached on the local disk, read access against data in /projects will typically no place too much of burden on the file server. The contents of the <code>/projects</code> folder will not be completely listed when e.g. executing <code>ls /projects/</code> as contents are mounted on demand by automounter. You can request a <code>/project</code> directory by talking to us on Mattermost or have your PI request one.</p> <p>The <code>/groups</code> directory is identical to <code>/projects</code> in technology. However every group on the organization has their own subdirectory.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#a-program-fails-with-the-message-too-many-open-files","title":"A program fails with the message <code>too many open files</code>","text":"<p>This limit can be changed for the current user session with <code>ulimit -Sn</code> followed by the desired number. For example:</p> <pre><code>ulimit -Sn 4096\n</code></pre> <p>It is advisable to execute the <code>ulimit</code> command only when a workflow requires keeping many files open at the same time. For example:</p> <pre><code>ulimit -Sn 4096 &amp;&amp; python mywork.py\n</code></pre>"},{"location":"troubleshooting/#fixing-permissions-for-shared-files","title":"Fixing permissions for shared files","text":"<p>If the permissions of files that are supposed to be shared are too restrictive, ask the owner to extend the permissions with:</p> <pre><code># Allow everyone to read everything in a directory.\nchmod -R a+r &lt;path to directory&gt;\n\n# Allow the owner's group to modify every file in a directory.\nchmod -R g+w &lt;path to directory&gt;\n\n# Extend the execution and directory browsing permissions that the owner has in a directory to everyone.\nfind &lt;path to directory&gt; -executable -exec chmod a+x {} \\;\n</code></pre>"},{"location":"troubleshooting/#gpu-memory-is-held-by-orphan-processes","title":"GPU memory is held by orphan processes","text":"<p>If processes are holding GPU memory while their parent terminates abnormally, they could be left with claimed memory and no work to do. In such a case, <code>nvidia-smi</code> will display occupied memory with no associated processes. The <code>fuser</code> command can help perform cleanup and reclaim GPU memory.</p> <p>For example, if <code>nvidia-smi</code> reports memory allocated in GPU 0, <code>fuser -v /dev/nvidia0</code> can display which processes are accessing it.</p> <pre><code>                USER    PID ACCESS COMMAND\n/dev/nvidia0:   bob  869108 F...m python\n                bob  1236874 F...m python\n                bob  1236922 F...m python\n</code></pre> <p>Users can only see processes owned by themselves. Administrators can run <code>fuser</code> as root to see all processes.</p> <p><code>fuser -k</code> sends a termination signal to the listed processes. It can be convenient as a last resort if certain processes are difficult to terminate cleanly.</p> <p>A typical troubleshooting sessions against leftover GPU memory might be as follows:</p> <ol> <li>Examine the output of <code>nvidia-smi</code> and look for GPUs with allocated memory but no processes attached.</li> <li>Execute <code>fuser -v /dev/nvidiaN</code> to list processes owned by you accessing the device (replace <code>N</code> with a GPU index).</li> <li>Save all important work, then try closing open programs cleanly.</li> <li>Execute <code>fuser -v /dev/nvidiaN</code> again.   If any processes show up and you don't have a way to terminate them cleanly, you can use <code>fuser -k /dev/nvidiaN</code> to kill them bluntly.</li> <li>Examine the output of <code>nvidia-smi</code> again.    If GPU memory is still occupied, ask an administrator to look into processes owned by other users.</li> </ol>"}]}